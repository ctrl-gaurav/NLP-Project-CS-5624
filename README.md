# NLPâ€‘Projectâ€‘CSâ€‘5624 â€” LLMThinkBench Experiments

<!-- [![Paper](https://img.shields.io/badge/Paper-PDF-blue)](LLMThinkBench___Project_Report.pdf) -->
[![Leaderboard](https://img.shields.io/badge/Live%20Leaderboard-Streamlit-orange)](https://llmthinkbench-leaderboard.streamlit.app/)
[![LLMThinkBench Package](https://img.shields.io/pypi/v/llmthinkbench)](https://pypi.org/project/llmthinkbench/)
[![GitHub stars](https://img.shields.io/github/stars/ctrl-gaurav/NLP-Project-CS-5624?style=social)](https://github.com/ctrl-gaurav/NLP-Project-CS-5624/stargazers)
[![License: MIT](https://img.shields.io/badge/License-MIT-yellow.svg)](https://opensource.org/licenses/MIT)

A course project for **CS 5624 (Natural Language Processing)**, Virginia Tech.

We measure how much large language models *overâ€‘think* on **basic** symbolicâ€‘reasoning tasks and explore three ways to keep answers short without losing accuracy:

1. **Prompt engineering** â€“ ask for the boxed answer only.  
2. **Tokenâ€‘budget caps** â€“ force every model to stay within a task-specific limit.  
3. **Distillation + SFT** â€“ fineâ€‘tune smaller Qwenâ€‘2.5 models on concise Geminiâ€‘2.0â€‘Flash traces.

All code builds on the openâ€‘sourced benchmark **[LLMThinkBench](https://github.com/ctrl-gaurav/LLMThinkBench)** and feeds scores to a public leaderboard.

---

## ğŸš€ Quick start

```bash
# clone the repo
git clone https://github.com/ctrl-gaurav/NLP-Project-CS-5624.git
cd NLP-Project-CS-5624

# install core dependencies (CUDA 12.x recommended)
conda create -n thinkbench python=3.10 -y
conda activate thinkbench
pip install -r requirements.txt          # torch, transformers, vllm, bitsandbytes â€¦
pip install llmthinkbench                # CLI + tasks as a PyPI package

# oneâ€‘line evaluation (example: Llamaâ€‘3â€‘Instructâ€‘8B on the SORT task)
llmthinkbench \
  --model meta-llama/Llama-3-8B-Instruct \
  --tasks sorting \
  --datapoints 1000 --folds 3 \
  --output-dir results/llama3_8b_sort
```

Fineâ€‘tuning scripts expect 4Ã— A100 80 GB GPUs but can be run on smaller hardware by lowering `--batch_size` and enabling gradient accumulation.

## ğŸ“Š Results Snapshot

| Model | Params | Avg. Accuracy â†‘ | Avg. Tokens â†“ |
|-------|--------|-----------------|---------------|
| Qwenâ€‘2.5â€‘3B (baseline) | 3 B | 74 % | 145 |
| Qwenâ€‘2.5â€‘3B + token cap | 3 B | 73 % | 71 |
| Qwenâ€‘2.5â€‘3B + SFT | 3 B | 91 % | 68 |

Full, alwaysâ€‘upâ€‘toâ€‘date numbers are available on the [leaderboard](https://llmthinkbench-leaderboard.streamlit.app/).

## ğŸ“ Repository Structure

| Path | Description |
|------|-------------|
| [`Prompt Engineering/`](./Prompt%20Engineering) | Prompt templates and helper scripts for directâ€‘answer vs. standard CoT prompting. Contains experimental prompt variations, ablation studies, and parameter sweeps. |
| [`RegularHyperParams/`](./RegularHyperParams) | Baseline runs with vanilla sampling settings (temperature=0.7, top_p=0.9). Includes model outputs and evaluation metrics across all benchmark tasks. |
| [`TruncatingBasedOnTradeoff/`](./TruncatingBasedOnTradeoff) | Experiments that search for the best accuracy/tokenâ€‘count tradeâ€‘off and then reâ€‘run models under those token caps. Contains analysis scripts and visualizations of optimal trade-off points. |
| [`benchmarking-website-results/`](./benchmarking-website-results) | JSON output files automatically ingested by the Streamlit leaderboard. These files contain structured results for all evaluated models. |
| [`dataset/`](./dataset) | Synthetic data used in the paper (global benchmark set and Gemini distillation set). Includes both training and evaluation splits across all reasoning tasks. |
| [`distillation + sft/`](./distillation%20%2B%20sft) | Training configs, LoRA adapters, and logs for supervised fineâ€‘tuning with Geminiâ€‘2.0â€‘Flash supervision. Contains hyperparameters, loss curves, and adapter weights. |
| [`important-scripts/`](./important-scripts) | Core CLI (`cli.py`), model loader, evaluation utilitiesâ€”entry points for most experiments. These are the backbone scripts used throughout the project. |
| [`llmthinkbench-tasks/`](./llmthinkbench-tasks) | All 14 benchmark tasks (sorting, counting, min/max, etc.) and their robust regex parsers. Each task has its own directory with problem generators and evaluation code. |
| [`plots.ipynb`](./plots.ipynb) | Jupyter notebook that reproduces all figures in the report (accuracy vs. tokens, tradeâ€‘off curves, etc.). |
| [`temp_and_top_p.zip`](./temp_and_top_p.zip) | Compressed results grid from the samplingâ€‘parameter sweep (temperature Ã— topâ€‘p). Contains detailed outputs from parameter studies. |
| `LICENSE` | MIT License â€” free to use and modify. |
| `README.md` | This file - project overview and documentation. |
| Misc. | `.gitignore`, `.DS_Store`, original README.md placeholder. |

## ğŸ“š Benchmark Tasks

LLMThinkBench includes 14 reasoning tasks across multiple categories:

### ğŸ”¢ Arithmetic
- Basic addition, subtraction, multiplication, division
- Multi-step arithmetic operations
- Sign recognition

### ğŸ§® Statistical
- Finding mean/average values
- Min/max identification
- Count operations
- Sum of series

### ğŸ”€ Algorithmic
- Sorting numbers
- Even/odd number identification
- Comparison operations
- Pattern completion

Each task evaluates both answer accuracy and verbosity (token count).

## ğŸ”¬ Methodology

Our approach explores different methods to reduce LLM verbosity while maintaining or improving accuracy:

1. **Prompt Engineering**:
   - Direct answer prompting ("Give only the boxed final answer")
   - Chain-of-thought ablations
   - Instruction variations

2. **Token Budget Enforcement**:
   - Task-specific token caps based on empirical accuracy trade-offs
   - Model-specific optimal truncation points

3. **Knowledge Distillation**:
   - Teacher-student training with Gemini-2.0-Flash as teacher
   - LoRA fine-tuning of Qwen-2.5 models
   - Direct supervision on concise yet accurate reasoning paths

## ğŸ”— Key Links

<!-- - [Paper PDF (project report)](LLMThinkBench___Project_Report.pdf) -->
- [Benchmark Code Repository](https://github.com/ctrl-gaurav/LLMThinkBench)
- [Live Leaderboard](https://llmthinkbench-leaderboard.streamlit.app/)
- [LLMThinkBench PyPI Package](https://pypi.org/project/llmthinkbench/)

## ğŸ“ˆ Live Benchmarking

You can contribute to our leaderboard by running:

```bash
# Run a full benchmark suite and upload results
llmthinkbench --model YOUR_MODEL_NAME --tasks all --upload-results

# Run specific tasks only
llmthinkbench --model YOUR_MODEL_NAME --tasks sorting,counting,mean --upload-results
```

Results will be automatically submitted to the leaderboard if the `--upload-results` flag is used.

## ğŸ§  Key Findings

- Large models spend significant computational effort "thinking" on tasks that could be solved more efficiently
- Token budget caps can reduce verbosity by 40-60% with minimal accuracy loss
- Distillation from concise teacher models provides the best accuracy/token trade-off
- Prompt engineering alone yields inconsistent results across model families
- Certain tasks (e.g., complex sorting) benefit more from explicit reasoning than others

<!-- See our [paper](LLMThinkBench___Project_Report.pdf) for detailed analysis and findings. -->

## ğŸ“ Citing

If you use this code or the benchmark in your research, please cite:

```bibtex
@misc{srivastava2025llmthinkbench,
  title   = {Are Language Models Overthinking on Simple Math Reasoning?},
  author  = {Srivastava, Gaurav and Hussain, Aafiya and Srinivasan, Sriram and Chauhan, Aninditaa},
  year    = {2025},
  url     = {https://github.com/ctrl-gaurav/LLMThinkBench}
}
```

## ğŸ‘¥ Authors

- **Gaurav Srivastava** 
- **Aafiya Hussain**
- **Sriram Srinivasan**
- **Aninditaa Chauhan**

For questions, please open an issue or contact us via the emails listed in the report.

## ğŸ“„ License

This project is licensed under the MIT License - see the [LICENSE](LICENSE) file for details.